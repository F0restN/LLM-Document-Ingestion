{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-12 @ 17:44:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.logger\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mLogger initialized successfully\u001b[0m\n",
      "\u001b[32m2025-01-12 @ 17:44:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36membedding.embedding_models\u001b[0m:\u001b[36mget_bge_embedding\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mUsing device: mps\u001b[0m\n",
      "\u001b[32m2025-01-12 @ 17:44:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36membedding.embedding_models\u001b[0m:\u001b[36mget_bge_embedding\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mLoading BGE embedding model: BAAI/bge-m3\u001b[0m\n",
      "\u001b[32m2025-01-12 @ 17:44:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36membedding.embedding_models\u001b[0m:\u001b[36mget_bge_embedding\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded BGE embedding model\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from embedding.embedding_models import EmbeddingModels\n",
    "from utils.logger import logger\n",
    "\n",
    "bgeEmbedding = EmbeddingModels().get_bge_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Aging care forum and Alzconnect forum data\n",
    "\n",
    "*Since, we need to process the Aging Care forum data first before chunking*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0           Should Someone With Dementia Be Driving?   \n",
      "1           Should Someone With Dementia Be Driving?   \n",
      "2           Should Someone With Dementia Be Driving?   \n",
      "3  Early Diagnosis of Alzheimer’s Is Crucial for ...   \n",
      "4  Early Diagnosis of Alzheimer’s Is Crucial for ...   \n",
      "\n",
      "                                            question  \\\n",
      "0    When Should Someone With Dementia Stop Driving?   \n",
      "1  Why is letting a senior with dementia drive da...   \n",
      "2   How to Stop a Person With Dementia from Driving?   \n",
      "3  How is caring for someone with Alzheimer’s dis...   \n",
      "4  What does the Alzheimer's treatment journey lo...   \n",
      "\n",
      "                                              answer  \\\n",
      "0  Driving is one of the most difficult issues th...   \n",
      "1  Wandering or “elopement” is a common behavior ...   \n",
      "2  Car keys are a symbol of independence for Amer...   \n",
      "3  AD poses real challenges not only for the peop...   \n",
      "4  While there are no treatments available that c...   \n",
      "\n",
      "                                                tags  \n",
      "0                \"Alzheimer's & Dementia\", 'Driving'  \n",
      "1                \"Alzheimer's & Dementia\", 'Driving'  \n",
      "2                \"Alzheimer's & Dementia\", 'Driving'  \n",
      "3  \"Alzheimer's & Dementia\", 'Diagnostic Criteria...  \n",
      "4  \"Alzheimer's & Dementia\", 'Diagnostic Criteria...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Aging care forum data\n",
    "agingcare_df = pd.read_json(\n",
    "    \"../../data/raw_content/knowledge/aging-care-forum.json\", orient=\"records\"\n",
    ")\n",
    "\n",
    "agingcare_df[\"question\"] = agingcare_df[\"question\"].astype(str)\n",
    "agingcare_df[\"answer\"] = agingcare_df[\"answer\"].astype(str)\n",
    "agingcare_df[\"tags\"] = agingcare_df[\"tags\"].astype(str).str.strip('[]')\n",
    "\n",
    "# remove duplicate entries\n",
    "agingcare_df.drop_duplicates(\n",
    "    subset=[\"answer\", \"question\"], \n",
    "    keep=\"first\", \n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "print(agingcare_df[['title','question','answer', 'tags']].head())\n",
    "\n",
    "# Alzconnect forum data\n",
    "alzconnect_df = pd.read_parquet(\n",
    "    \"../../data/raw_content/knowledge/alz-connect-forum.parquet\"\n",
    ")\n",
    "\n",
    "# print(alzconnect_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursively split the documents into chunks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-12 @ 17:44:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mChunking Aging Care forum data\u001b[0m\n",
      "\u001b[32m2025-01-12 @ 17:44:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mChunking Alzconnect forum data\u001b[0m\n",
      "5014\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import (\n",
    "    # CharacterTextSplitter,\n",
    "    # MarkdownHeaderTextSplitter,\n",
    "    # MarkdownTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "agingcare_document_chunks = []\n",
    "alzconnect_document_chunks = []\n",
    "\n",
    "logger.info(\"Chunking Aging Care forum data\")\n",
    "for doc in agingcare_df.itertuples():\n",
    "    for chunk in text_splitter.split_text(\" \".join([doc.question,doc.answer])):\n",
    "        agingcare_document_chunks.append(\n",
    "            Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"source\": doc.url,\n",
    "                    \"title\": doc.title,\n",
    "                    \"author\": getattr(doc, \"author\", \"Unknown\"),\n",
    "                    \"tag\": getattr(doc, \"tags\", \"Unknown\"),\n",
    "                    \"source-tag\": \"agingcare\",\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "        \n",
    "logger.info(\"Chunking Alzconnect forum data\")\n",
    "for doc in alzconnect_df.itertuples():\n",
    "    for chunk in text_splitter.split_text(\" \".join([doc.title,doc.text])):\n",
    "        alzconnect_document_chunks.append(\n",
    "            Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"source\": doc.source,\n",
    "                    \"title\": doc.title,\n",
    "                    \"author\": getattr(doc, \"author\", \"Unknown\"),\n",
    "                    \"tag\": getattr(doc, \"tags\", \"Unknown\"),\n",
    "                    \"source-tag\": \"alzconnect\",\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Merge all chunks \n",
    "peer_support_document_chunks = []\n",
    "for group in [agingcare_document_chunks, alzconnect_document_chunks]:\n",
    "    peer_support_document_chunks.extend(group)\n",
    "\n",
    "type(peer_support_document_chunks)\n",
    "\n",
    "print(len(agingcare_document_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate vector store for peer support knowledgebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Chroma vector store\n",
    "# TODO Import function from vector_store.py that builds a Chroma vector store\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from embedding.vector_store import VectorStore\n",
    "\n",
    "vectorstore_path = \"../../data/vector_database/peer_kb\"\n",
    "force_rebuild = True\n",
    "\n",
    "\n",
    "def build_peer_support_vectorstore(vectorstore_path: str, force_rebuild: bool):\n",
    "    if os.path.isdir(vectorstore_path) and force_rebuild:\n",
    "        shutil.rmtree(vectorstore_path)\n",
    "        logger.info(f\"Vector store {vectorstore_path} already exists and force_rebuild is True. Rebuilding...\")\n",
    "    \n",
    "    try:\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=peer_support_document_chunks,\n",
    "            embedding=bgeEmbedding,\n",
    "            persist_directory=vectorstore_path,\n",
    "        )\n",
    "        logger.info(f\"Vector store built successfully at {vectorstore_path}\")\n",
    "        # return vectorstore\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to build vector store: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# build_peer_support_vectorstore(vectorstore_path, force_rebuild)\n",
    "\n",
    "# peer_support_kb =VectorStore().build_chroma_vectorstore(\n",
    "#     docs=agingcare_document_chunks,\n",
    "#     embedding_model=bgeEmbedding,\n",
    "#     collection_name=\"peer_support_knowledgebase\",\n",
    "#     vectorstore_path=\"../../data/vector_database/peer_kb\",\n",
    "#     force_rebuild=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--- \n",
    "\n",
    "### Process research paper data\n",
    "\n",
    "Load pubmed data and process it. For each document, we straighten the array and convert it to a string since the array is not a valid in Langchain document metadata type.\n",
    "\n",
    "And then we chunk it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pubmed data\n",
    "import json\n",
    "from utils.tools import ToolKits\n",
    "\n",
    "tools = ToolKits()\n",
    "\n",
    "pubmed_path = \"../../data/raw_content/knowledge/pubmed-central-delirium-family-caregiving.json\"\n",
    "with open(pubmed_path, 'r') as file:\n",
    "    pubmed_data = json.load(file)\n",
    "\n",
    " # Chunk pubmed data\n",
    "pubmed_document_chunks = []\n",
    "\n",
    "def remove_square_brackets(text: str) -> str:\n",
    "    return text.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "\n",
    "try:\n",
    "    for doc in pubmed_data:\n",
    "        if doc['abstract'] != \"\":\n",
    "            for chunk in text_splitter.split_text(doc['title'] + \" \" + doc['abstract']):\n",
    "                if chunk != \"\":\n",
    "                    pubmed_document_chunks.append(\n",
    "                        Document(\n",
    "                            page_content=chunk,\n",
    "                            metadata={\n",
    "                                \"source\": doc['url'],\n",
    "                                \"title\": doc['title'],\n",
    "                                \"author\": tools.straight_array_to_string(doc['authors']),\n",
    "                                \"tag\": '',\n",
    "                                \"source-tag\": \"pubmed\",\n",
    "                            },\n",
    "                        )\n",
    "                    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to chunk pubmed data: {str(e)}\")\n",
    "\n",
    "print(len(pubmed_document_chunks))\n",
    "print(pubmed_document_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vector store for pubmed data\n",
    "\n",
    "Use key word \"delirium\" to test the vector store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from embedding.vector_store import VectorStore\n",
    "\n",
    "vs = VectorStore();\n",
    "\n",
    "print(vs)\n",
    "\n",
    "pubmed_kb = vs.build_chroma_vectorstore(\n",
    "    docs=pubmed_document_chunks,\n",
    "    embedding_model=bgeEmbedding,\n",
    "    # collection_name=\"research_knowledgebase\",\n",
    "    vectorstore_path=\"../../data/vector_database/research_kb\",\n",
    "    force_rebuild=True\n",
    ")\n",
    "\n",
    "print(pubmed_kb.similarity_search(\"delirium\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
